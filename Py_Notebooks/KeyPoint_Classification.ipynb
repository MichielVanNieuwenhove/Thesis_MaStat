{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "890e1953-4664-4865-96d4-8aafa1941d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "26669a37-d9d0-48fd-9998-b10d17b14030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "SEED = 42\n",
    "use_gpu = False\n",
    "num_classes = 2\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ba91993c-fc5d-41de-9a5c-bcb26f321bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed057f9-9d09-44e3-adb7-71d382892513",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3b6d977b-0a29-4281-9040-ce720168320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kp_labeled = pd.read_feather('../Data/TrainingData/df_kp_labeled.feather')\n",
    "df_kp_unlabeled = pd.read_feather('../Data/TrainingData/df_kp_unlabeled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f067021d-8fea-4926-a041-c20f1f469eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID_kp  ID_dta                                          KEY_POINT  \\\n",
      "0      6      50  Stress stemmed from the fear of disease progre...   \n",
      "1      8      50   Not psychologically prepared to manage such s...   \n",
      "2     16      54  Not psychologically prepared to manage these s...   \n",
      "3     24      60  Stopping treatment was seen as a sign of recov...   \n",
      "4     35      68  The expertise of GPs familiar with Crohn's cou...   \n",
      "\n",
      "                                KEY_POINT_normalized  ID_kp_distinct  label  \n",
      "0  stress stemmed from the fear of disease progre...               3      1  \n",
      "1  not psychologically prepared to manage such so...               5      0  \n",
      "2  not psychologically prepared to manage these s...              11      0  \n",
      "3  stopping treatment was seen as a sign of recov...              16      1  \n",
      "4  the expertise of gps familiar with crohn's cou...              26      1  \n"
     ]
    }
   ],
   "source": [
    "print(df_kp_labeled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2167b604-ef4c-4775-9742-af5f20c61010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID_kp                 19 non-null     int32 \n",
      " 1   ID_dta                19 non-null     int32 \n",
      " 2   KEY_POINT             19 non-null     object\n",
      " 3   KEY_POINT_normalized  19 non-null     object\n",
      " 4   ID_kp_distinct        19 non-null     int64 \n",
      " 5   label                 19 non-null     int64 \n",
      "dtypes: int32(2), int64(2), object(2)\n",
      "memory usage: 888.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_kp_labeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "88f1080a-02cb-4752-a20b-a02dbeaf53dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109 entries, 0 to 108\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID_kp                 109 non-null    int32 \n",
      " 1   ID_dta                109 non-null    int32 \n",
      " 2   KEY_POINT             109 non-null    object\n",
      " 3   KEY_POINT_normalized  109 non-null    object\n",
      " 4   ID_kp_distinct        109 non-null    int64 \n",
      "dtypes: int32(2), int64(1), object(2)\n",
      "memory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_kp_unlabeled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6d688-7aec-4813-a519-eae755755412",
   "metadata": {},
   "source": [
    "# Importing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "14e13c95-61a2-4497-9cb9-23561037bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "200e9106-8532-4e05-b4d3-326647bb7372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "device = \"cpu\"\n",
    "if use_gpu:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495451-8f76-4ca0-bf96-fed8fe6cc7fd",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e673f7-a80e-4208-ae32-57ca94700d9c",
   "metadata": {},
   "source": [
    "The data is normalized as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b587bcdf-bf5b-48dd-b6ee-9f507fac3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kp['KEY_POINT_normalized'] = df_kp['KEY_POINT'].str.strip().str.replace('\\n', ' ').str.replace('\\r', ' ').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762de105-d65b-4d4b-8ce0-82bec323e989",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "37d5181b-0c21-4380-a5a0-e697d4c2369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tokenized input length: 65\n"
     ]
    }
   ],
   "source": [
    "all_keypoints = df_kp_labeled['KEY_POINT_normalized'].tolist() + df_kp_unlabeled['KEY_POINT_normalized'].tolist()\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    df_kp_labeled['KEY_POINT_normalized'].tolist(),\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "# Calc max length\n",
    "lengths = [len(input_ids) for input_ids in tokenized['input_ids']]\n",
    "max_length = max(lengths)\n",
    "print(f\"Maximum tokenized input length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "25c2f87c-81ab-4e4c-9e9b-fe6dd2838c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"KEY_POINT_normalized\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "dataset_labeled = Dataset.from_pandas(df_kp_labeled)\n",
    "dataset_unlabeled = Dataset.from_pandas(df_kp_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "05688bfa-54dd-4ebe-8b1f-44dff558911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6335cbd6f42c4bb187a35668a8aa0aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           input_ids  \\\n",
      "0  [101, 6911, 27674, 2013, 1996, 3571, 1997, 429...   \n",
      "1  [101, 2025, 8317, 2135, 4810, 2000, 6133, 2107...   \n",
      "2  [101, 2025, 8317, 2135, 4810, 2000, 6133, 2122...   \n",
      "3  [101, 7458, 3949, 2001, 2464, 2004, 1037, 3696...   \n",
      "4  [101, 1996, 11532, 1997, 14658, 5220, 2007, 13...   \n",
      "\n",
      "                                      attention_mask  label  ID_kp_distinct  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1               3  \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...      0               5  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...      0              11  \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1              16  \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1              26  \n"
     ]
    }
   ],
   "source": [
    "# Labeled\n",
    "df_tokenized_labeled = dataset_labeled.map(tokenize_fn, batched=True)\n",
    "df_tokenized_labeled_pd = df_tokenized_labeled.to_pandas()\n",
    "df_tokenized_labeled_pd['label'] = df_tokenized_labeled_pd['label'].astype(int)\n",
    "\n",
    "df_tokenized_labeled_pd = df_tokenized_labeled_pd[['input_ids', 'attention_mask', 'label', 'ID_kp_distinct']]\n",
    "print(df_tokenized_labeled_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1516e158-3859-432a-9c70-1ababf2255a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_tokenized_unlabeled = dataset_unlabeled.map(tokenize_fn, batched=True)\\ndf_tokenized_unlabeled_pd = df_tokenized_unlabeled.to_pandas()\\ndf_tokenized_unlabeled_pd['label'] = None\\n\\ndf_tokenized_unlabeled_pd = df_tokenized_unlabeled_pd[['input_ids', 'attention_mask', 'label', 'ID_kp_distinct']]\\nprint(df_tokenized_unlabeled_pd.head())\\n\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlabeled\n",
    "'''\n",
    "df_tokenized_unlabeled = dataset_unlabeled.map(tokenize_fn, batched=True)\n",
    "df_tokenized_unlabeled_pd = df_tokenized_unlabeled.to_pandas()\n",
    "df_tokenized_unlabeled_pd['label'] = None\n",
    "\n",
    "df_tokenized_unlabeled_pd = df_tokenized_unlabeled_pd[['input_ids', 'attention_mask', 'label', 'ID_kp_distinct']]\n",
    "print(df_tokenized_unlabeled_pd.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28eb69b-350e-4be3-bdc3-98925c2afc23",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0845703-b3b1-402e-a3a8-03e6b1ecf38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a4ce03-c843-4582-b115-35e0cb45e723",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15c7f3-d547-4dfb-b674-76cba957bb5e",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "486443da-34dd-4455-b821-465f2509f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df_tokenized_labeled_pd,\n",
    "    test_size=0.3,\n",
    "    stratify=df_tokenized_labeled_pd['label'],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b26f80-c1bb-4434-9c0d-6414a82b9913",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b414c706-5404-4916-b3fd-3e4de9101ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/base_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    save_steps=1,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "85d79fd2-eb10-4bf0-8436-3fd7a9af0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7e104432-161b-4149-b7c7-337ee8991f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.636068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.640951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.647095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.654297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.665975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.676554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.685506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.693319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.697428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.699219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.5348620533943176, metrics={'train_runtime': 15.9964, 'train_samples_per_second': 8.127, 'train_steps_per_second': 0.625, 'total_flos': 4342360191000.0, 'train_loss': 0.5348620533943176, 'epoch': 10.0})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca0366-51f9-43ad-9779-d3e09066fd66",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "844f5d4f-eb64-4950-b02a-b45071459168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1]\n",
      "[0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "predicted_logits = predictions.predictions\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "predicted_labels = np.argmax(predicted_logits, axis=-1)\n",
    "print(predicted_labels)\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d07a6c-7aa1-45b8-a0ec-835f46e4a770",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b7d47f7-c62d-45cb-8417-c9d03f12d386",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_labeled_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gold_data \u001b[38;5;241m=\u001b[39m \u001b[43mfull_labeled_set\u001b[49m        \u001b[38;5;66;03m# 19 examples\u001b[39;00m\n\u001b[0;32m      2\u001b[0m unlabeled \u001b[38;5;241m=\u001b[39m full_unlabeled_set      \u001b[38;5;66;03m# ~100 examples\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pseudo_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_labeled_set' is not defined"
     ]
    }
   ],
   "source": [
    "gold_data = full_labeled_set        # 19 examples\n",
    "unlabeled = full_unlabeled_set      # ~100 examples\n",
    "pseudo_data = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Train on gold + retained pseudo\n",
    "    train_set = gold_data + pseudo_data\n",
    "    trainer.train(train_dataset=train_set)\n",
    "\n",
    "    # 2. Predict on a small new batch of unlabeled\n",
    "    batch = unlabeled.sample(n=3, replace=False)\n",
    "    logits = trainer.predict(batch).logits\n",
    "    probs = softmax(logits)\n",
    "    preds = argmax(logits)\n",
    "\n",
    "    # 3. Select highâ€‘confidence pseudolabels\n",
    "    mask_high = probs.max(axis=1) > 0.9\n",
    "    new_pseudo = batch[mask_high].add_column(\"labels\", preds[mask_high])\n",
    "\n",
    "    # 4. Update pseudo_data:\n",
    "    #    - Keep old ones with prob > 0.95\n",
    "    #    - Add new ones\n",
    "    pseudo_data = [\n",
    "      x for x in pseudo_data if x.confidence > 0.95\n",
    "    ] + new_pseudo\n",
    "\n",
    "    # 5. Remove used unlabeled examples\n",
    "    unlabeled = unlabeled.drop(batch.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0bb79-158d-4608-8263-f8fe2cd23b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b705e998-0a02-4e11-9ae2-e424fa655cae",
   "metadata": {},
   "source": [
    "# Simulating CI (Clopper-Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16d4af-8576-4bcf-84fa-806c388cce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10     # correct predictions\n",
    "n = 10      # total predictions\n",
    "\n",
    "ci_low, ci_high = proportion_confint(k, n, alpha=0.05, method=\"beta\")\n",
    "print(f\"95% CI: [{ci_low:.2%}, {ci_high:.2%}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_thesis_39",
   "language": "python",
   "name": "env_thesis_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
